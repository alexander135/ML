{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.22099304199219\n",
      "Clip gradient :  5.336664428366323\n",
      "53.484554290771484\n",
      "Clip gradient :  6.0637077022784664\n",
      "33.196739196777344\n",
      "Clip gradient :  7.760874881240009\n",
      "23.45627212524414\n",
      "Clip gradient :  6.269695517837148\n",
      "16.426959991455078\n",
      "Clip gradient :  5.490024063175561\n",
      "12.70097541809082\n",
      "Clip gradient :  5.671535515397334\n",
      "11.786163330078125\n",
      "Clip gradient :  10.215879641553132\n",
      "10.311821937561035\n",
      "Clip gradient :  5.137266996127918\n",
      "8.896644592285156\n",
      "Clip gradient :  4.418323277850166\n",
      "7.8319172859191895\n",
      "Clip gradient :  4.13546127992571\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию LSTM и обучить предсказывать слово\n",
    "class my_LSTM(nn.Module):\n",
    "    def __init__(self, in_size = 5, hidden_size = 3, out_size = 5):\n",
    "        super (my_LSTM,self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.hidden1 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden2 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.hidden3 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden4 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.hidden5 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden6 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.hidden7 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden8 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "        self.c = 1\n",
    "        \n",
    "    def forward(self, x, prev_hidden):\n",
    "        inp = self.act(self.hidden1(x) + self.hidden2(prev_hidden))\n",
    "        forget = self.act(self.hidden3(x) + self.hidden4(prev_hidden))\n",
    "        outp = self.act(self.hidden5(x) + self.hidden6(prev_hidden))\n",
    "        candidate = self.activation(self.hidden7(x) + self.hidden8(prev_hidden))\n",
    "        self.c = forget * self.c + inp * candidate\n",
    "        hidden = outp * self.activation(self.c)\n",
    "        y = self.outweight(hidden)\n",
    "        return y, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.35601806640625\n",
      "Clip gradient :  3.638524761829421\n",
      "67.72064971923828\n",
      "Clip gradient :  2.6554060415142775\n",
      "51.04287338256836\n",
      "Clip gradient :  10.773632401020107\n",
      "37.76627731323242\n",
      "Clip gradient :  7.6381911556386015\n",
      "36.466697692871094\n",
      "Clip gradient :  13.534629251986109\n",
      "57.38458251953125\n",
      "Clip gradient :  289.3133207750561\n",
      "24.829696655273438\n",
      "Clip gradient :  8.112080340346823\n",
      "35.39712905883789\n",
      "Clip gradient :  18.44751070512961\n",
      "72.78892517089844\n",
      "Clip gradient :  40.53078834535044\n",
      "50.70270919799805\n",
      "Clip gradient :  37.61302017453754\n",
      "41.362430572509766\n",
      "Clip gradient :  33.535560683172804\n",
      "33.36905288696289\n",
      "Clip gradient :  67.50821384937122\n",
      "39.76215744018555\n",
      "Clip gradient :  24.718746808045672\n",
      "29.332265853881836\n",
      "Clip gradient :  4.486298770500658\n",
      "23.87061309814453\n",
      "Clip gradient :  8.021148938498106\n",
      "23.724761962890625\n",
      "Clip gradient :  24.799826249609538\n",
      "25.477750778198242\n",
      "Clip gradient :  11.715265105275524\n",
      "33.39739227294922\n",
      "Clip gradient :  23.37700325446712\n",
      "23.16580581665039\n",
      "Clip gradient :  11.946776939613734\n",
      "28.470592498779297\n",
      "Clip gradient :  23.418233183560243\n",
      "38.96354675292969\n",
      "Clip gradient :  547.6578521874932\n",
      "30.980987548828125\n",
      "Clip gradient :  10.417621041878446\n",
      "33.56779479980469\n",
      "Clip gradient :  14.299846389862148\n",
      "28.46872901916504\n",
      "Clip gradient :  26.37206053130597\n",
      "20.648969650268555\n",
      "Clip gradient :  3.6941478854806737\n",
      "17.748371124267578\n",
      "Clip gradient :  4.018813397574526\n",
      "15.69589900970459\n",
      "Clip gradient :  5.714551116490331\n",
      "36.445701599121094\n",
      "Clip gradient :  84.04206777674825\n",
      "28.736568450927734\n",
      "Clip gradient :  376.2782133518455\n",
      "19.322011947631836\n",
      "Clip gradient :  9.20178637142981\n",
      "15.554447174072266\n",
      "Clip gradient :  6.889243408229968\n",
      "17.523117065429688\n",
      "Clip gradient :  3.984827592168535\n",
      "19.109092712402344\n",
      "Clip gradient :  15.397626412028028\n",
      "15.094408988952637\n",
      "Clip gradient :  2.8160870365233097\n",
      "12.73678207397461\n",
      "Clip gradient :  2.1725616323116066\n",
      "10.134918212890625\n",
      "Clip gradient :  2.400825082856271\n",
      "8.356565475463867\n",
      "Clip gradient :  0.8631584190167624\n",
      "7.257925987243652\n",
      "Clip gradient :  1.1620532803516122\n",
      "6.531631946563721\n",
      "Clip gradient :  1.9476207090985034\n",
      "5.993912696838379\n",
      "Clip gradient :  2.158767638382455\n",
      "5.788594722747803\n",
      "Clip gradient :  3.089218677808451\n",
      "5.275119781494141\n",
      "Clip gradient :  1.3428888455166217\n",
      "5.1229705810546875\n",
      "Clip gradient :  2.3413807848002097\n",
      "4.7821760177612305\n",
      "Clip gradient :  2.013671090455905\n",
      "4.539856910705566\n",
      "Clip gradient :  2.262210332447052\n",
      "4.285124778747559\n",
      "Clip gradient :  2.403623448432652\n",
      "4.005962371826172\n",
      "Clip gradient :  2.3249447286772877\n",
      "3.755826950073242\n",
      "Clip gradient :  0.6313446538671766\n",
      "3.93027400970459\n",
      "Clip gradient :  3.6811599411676506\n",
      "3.6348495483398438\n",
      "Clip gradient :  2.2875630403248546\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "ds = WordDataSet(word=word)\n",
    "rnn = my_LSTM(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden2.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden2.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово\n",
    "#Написать реализацию LSTM и обучить предсказывать слово\n",
    "class my_GRU(nn.Module):\n",
    "    def __init__(self, in_size = 5, hidden_size = 3, out_size = 5):\n",
    "        super (my_GRU,self).__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.hidden1 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden2 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.hidden3 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden4 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.hidden5 = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden6 = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x, prev_hidden):\n",
    "        update = self.act(self.hidden1(x) + self.hidden2(prev_hidden))\n",
    "        reset = self.act(self.hidden3(x) + self.hidden4(prev_hidden))\n",
    "        candidate = self.activation(self.hidden5(x) + self.hidden6(reset * prev_hidden))\n",
    "        h = (1- update) * candidate + update * prev_hidden\n",
    "        y = self.outweight(h)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.98716735839844\n",
      "Clip gradient :  4.988471649295691\n",
      "58.69424819946289\n",
      "Clip gradient :  5.933613025097945\n",
      "23.030250549316406\n",
      "Clip gradient :  4.588978697611059\n",
      "6.0640974044799805\n",
      "Clip gradient :  3.428807601401312\n",
      "2.248671054840088\n",
      "Clip gradient :  3.3912346983126307\n",
      "0.9059104919433594\n",
      "Clip gradient :  1.6446052162852034\n",
      "0.24412250518798828\n",
      "Clip gradient :  0.23546975280878418\n",
      "0.11685466766357422\n",
      "Clip gradient :  0.09932252023226443\n",
      "0.07844161987304688\n",
      "Clip gradient :  0.05890358912067171\n",
      "0.05939197540283203\n",
      "Clip gradient :  0.03783246439918696\n",
      "0.048636436462402344\n",
      "Clip gradient :  0.025589646201160485\n",
      "0.042057037353515625\n",
      "Clip gradient :  0.021814562185764358\n",
      "0.03733539581298828\n",
      "Clip gradient :  0.01905125975721383\n",
      "0.033677101135253906\n",
      "Clip gradient :  0.01717678641427834\n",
      "0.030701637268066406\n",
      "Clip gradient :  0.015689288738526964\n",
      "0.028219223022460938\n",
      "Clip gradient :  0.014483432413952245\n",
      "0.026108741760253906\n",
      "Clip gradient :  0.013484172846603415\n",
      "0.024278640747070312\n",
      "Clip gradient :  0.012620307009219374\n",
      "0.02268218994140625\n",
      "Clip gradient :  0.011871223730586314\n",
      "0.02127361297607422\n",
      "Clip gradient :  0.011211719699141823\n",
      "0.020010948181152344\n",
      "Clip gradient :  0.01061994522650982\n",
      "0.018881797790527344\n",
      "Clip gradient :  0.01008931615958773\n",
      "0.017862319946289062\n",
      "Clip gradient :  0.009605552989010618\n",
      "0.01694202423095703\n",
      "Clip gradient :  0.009163233288287184\n",
      "0.016101837158203125\n",
      "Clip gradient :  0.008750745282910696\n",
      "0.015336036682128906\n",
      "Clip gradient :  0.008366736077419383\n",
      "0.014636039733886719\n",
      "Clip gradient :  0.008005294823033055\n",
      "0.013995170593261719\n",
      "Clip gradient :  0.007666722968549932\n",
      "0.013402938842773438\n",
      "Clip gradient :  0.007345448900840319\n",
      "0.012869834899902344\n",
      "Clip gradient :  0.00705141312538208\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "ds = WordDataSet(word=word)\n",
    "rnn = my_GRU(in_size=ds.vec_size, hidden_size=10, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 300\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden2.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y ,hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden2.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
